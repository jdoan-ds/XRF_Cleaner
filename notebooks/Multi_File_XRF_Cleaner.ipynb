{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486be55b-984c-44d4-9a66-7417a8cfe73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Clean start ----\n",
    "try:\n",
    "    del pandas\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2652a4-a795-45f2-a255-fdfc9d354d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Imports ----\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90d4770-23b0-49a6-96f4-6bf9c93b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Set up folders ----\n",
    "input_folder = '.'  # You can change this to another folder path\n",
    "output_folder = './processed/'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b71d992c-c068-4fac-88fb-3b483c9eb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Log file path ----\n",
    "log_file_path = os.path.join(output_folder, 'processing_log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50eebaa8-1f8f-4514-81dd-5688c766a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Serial-to-Well Name Mapping ----\n",
    "serial_to_well = {\n",
    "    '841515': 'Sample Data',\n",
    "    # Add more serial numbers and well names as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b288a4f1-3557-4231-88b0-a31d42d0a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Dry run mode ----\n",
    "dry_run = False  # Set to True to preview files without modifying anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1b7d31-5297-4645-897c-5488c573363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Track processed files to avoid duplicates ----\n",
    "if os.path.exists(log_file_path):\n",
    "    with open(log_file_path, 'r') as f:\n",
    "        processed_files = set(line.strip() for line in f if line.strip())\n",
    "else:\n",
    "    processed_files = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce85a8ef-ba8b-4947-913c-928f7b7ba68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Stats for dashboard ----\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0653cc-5b30-4efc-94d9-ad68faf001f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Per-well master dataframes cache ----\n",
    "master_dataframes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d359bd9-b4af-4f65-a013-b360be95b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Process files ----\n",
    "for filepath in filepaths:\n",
    "    try:\n",
    "        filename = os.path.basename(filepath)\n",
    "\n",
    "        if filename in processed_files:\n",
    "            print(f\"‚è≠Ô∏è Skipping already processed file: {filename}\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß™ Processing: {filename}\")\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Drop empty rows and columns\n",
    "        df.dropna(axis=0, how='all', inplace=True)\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        # Clean headers\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        # Convert wt% to ppm for known elements\n",
    "        for element in ['Ca', 'K', 'Fe']:\n",
    "            if element in df.columns and df[element].max() < 1:\n",
    "                df[element + '_ppm'] = df[element] * 10000\n",
    "\n",
    "        # Replace zero in common ratio denominators\n",
    "        for col in ['Al', 'Ti']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].replace(0, 0.0001)\n",
    "\n",
    "        # Trim extreme outliers\n",
    "        for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "            q_low = df[col].quantile(0.01)\n",
    "            q_hi = df[col].quantile(0.99)\n",
    "            df[col] = df[col].clip(lower=q_low, upper=q_hi)\n",
    "\n",
    "        # Extract well name from serial\n",
    "        serial = filename.split('-')[1]\n",
    "        well_name = serial_to_well.get(serial, 'UNKNOWN_WELL')\n",
    "\n",
    "        # Add source file label\n",
    "        df['source_file'] = filename\n",
    "\n",
    "        # Cache and concatenate into master dataframe in memory\n",
    "        if well_name not in master_dataframes:\n",
    "            master_file_path = os.path.join(output_folder, f'master_{well_name}.csv')\n",
    "            if os.path.exists(master_file_path):\n",
    "                master_dataframes[well_name] = pd.read_csv(master_file_path)\n",
    "            else:\n",
    "                master_dataframes[well_name] = pd.DataFrame()\n",
    "\n",
    "        master_dataframes[well_name] = pd.concat([master_dataframes[well_name], df], ignore_index=True)\n",
    "        processed_count += 1\n",
    "\n",
    "        if not dry_run:\n",
    "            # Save cleaned individual file\n",
    "            output_file = os.path.join(output_folder, f'processed_{filename}')\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"‚úÖ Saved cleaned file to: {output_file}\")\n",
    "\n",
    "            # Log the processed file\n",
    "            with open(log_file_path, 'a') as log:\n",
    "                log.write(filename + '\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "        errors.append((filename, str(e)))\n",
    "\n",
    "# ---- Save updated master files ----\n",
    "if not dry_run:\n",
    "    for well_name, df in master_dataframes.items():\n",
    "        df = df.copy()  # De-fragment for performance\n",
    "        master_path = os.path.join(output_folder, f'master_{well_name}.csv')\n",
    "        df.to_csv(master_path, index=False)\n",
    "        print(f\"üìÅ Updated master sheet for: {well_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae7a4a-64e0-4162-85cf-a2de1da6f78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc03af3b-a34e-4cb8-9e80-587775bd12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Per-well master dataframes cache ----\n",
    "master_dataframes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a89981-af30-4b28-aab4-9dccba82034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- QA/QC Summary Dashboard ----\n",
    "print(\"\\nüßæ Run Summary\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total files found: {len(filepaths)}\")\n",
    "print(f\"Files processed: {processed_count}\")\n",
    "print(f\"Files skipped: {skipped_count}\")\n",
    "print(f\"Files errored: {len(errors)}\")\n",
    "if errors:\n",
    "    for fname, msg in errors:\n",
    "        print(f\"‚ùå {fname}: {msg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
